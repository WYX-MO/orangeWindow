{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10f7e0f6",
   "metadata": {},
   "source": [
    "## 题解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb201788",
   "metadata": {},
   "source": [
    "### mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5760c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1ea87c",
   "metadata": {},
   "source": [
    "- 先查看数据的基本情况,检查缺失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6592b71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取训练集和测试集\n",
    "train_df = pd.read_csv('E:\\\\download\\\\AI派2025招新第一轮测试\\\\AI派2025招新第一轮测试\\\\MBAAdmission\\\\train.csv')  \n",
    "test_df = pd.read_csv('E:\\\\download\\\\AI派2025招新第一轮测试\\\\AI派2025招新第一轮测试\\\\MBAAdmission\\\\test.csv')\n",
    "\n",
    "print(train_df.head())\n",
    "# print(train_df.tail())\n",
    "\n",
    "print(f\"训练集形状: {train_df.shape}\")  #\n",
    "print(f\"测试集形状: {test_df.shape}\")  #\n",
    "\n",
    "print(\"特征列名: \", train_df.columns.tolist()) #\n",
    "\n",
    "print(train_df.info())  # info方法查看类型和缺失值\n",
    "print(test_df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d836e6",
   "metadata": {},
   "source": [
    "- 先对已有数据求出application_id,gpa,gmat,work_exp基本的统计信息,方便填入缺失的信息.\n",
    "类别型特征的缺失值常用众数填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4299a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.isnull().sum())\n",
    "# 例如：race 列可能因国际学生无数据而出现缺失\n",
    "# # 对数值列（如 gpa、gmat、work_exp）进行统计描述\n",
    "print(train_df.describe())  # 包含均值、标准差、最小值、最大值、分位数等\n",
    "\n",
    "non_num_cols = train_df.select_dtypes(include=['object'])\n",
    "#non_num_cols_test = test_df.select_dtypes(include=['object'])\n",
    "\n",
    "print(non_num_cols.count())\n",
    "non_num_cols = non_num_cols.columns.tolist()\n",
    "#non_num_cols_ = non_num_cols_.colums.tolist()\n",
    "\n",
    "for col in non_num_cols:\n",
    "    print(f\"===== 特征：{col} =====\")\n",
    "    \n",
    "    # 获取唯一类别（含缺失值可保留）\n",
    "    unique_categories = train_df[col].unique()\n",
    "    print(f\"唯一类别（共{len(unique_categories)}种）：{unique_categories}\")\n",
    "    #temp[\"唯一类别数\"].append(len(unique_categories))\n",
    "    \n",
    "    # 获取每个类别的出现次数（排除缺失值，按降序排列）\n",
    "    value_counts = train_df[col].value_counts(dropna=False)  # dropna=False包含缺失值\n",
    "    #temp[\"类别\"].append(value_counts.keys().tolist())\n",
    "    print(\"类别出现次数：\")\n",
    "    print(value_counts)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")  # 分隔线\n",
    "# 按逻辑填补缺失值\n",
    "train_df['gender'] = train_df['gender'].fillna(train_df['gender'].mode()[0])\n",
    "train_df['international'] = train_df['international'].fillna(train_df['international'].mode()[0])\n",
    "train_df['gpa'] = train_df['gpa'].fillna(train_df['gpa'].mean())\n",
    "train_df['major'] = train_df['major'].fillna(train_df['major'].mode()[0])\n",
    "train_df['race'] = train_df['race'].fillna(train_df['race'].mode()[0])\n",
    "train_df['gmat'] = train_df['gmat'].fillna(train_df['gmat'].median())\n",
    "train_df['work_exp'] = train_df['work_exp'].fillna(0)\n",
    "train_df['work_industry'] = train_df['work_industry'].fillna('other')\n",
    "\n",
    "test_df['gender'] = test_df['gender'].fillna(test_df['gender'].mode()[0])\n",
    "test_df['international'] = test_df['international'].fillna(test_df['international'].mode()[0])\n",
    "test_df['gpa'] = test_df['gpa'].fillna(test_df['gpa'].mean())\n",
    "test_df['major'] = test_df['major'].fillna(test_df['major'].mode()[0])\n",
    "test_df['race'] = test_df['race'].fillna(test_df['race'].mode()[0])\n",
    "test_df['gmat'] = test_df['gmat'].fillna(test_df['gmat'].median())\n",
    "test_df['work_exp'] = test_df['work_exp'].fillna(0)\n",
    "test_df['work_industry'] = test_df['work_industry'].fillna('other')\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "\n",
    "# 注：mode() 返回 Series，取 [0] 获取第一个众数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对分类特征进行one-hot编码\n",
    "# 首先确定需要编码的分类列（通常是object类型的列）\n",
    "categorical_cols = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_cols_ = test_df.select_dtypes(include=['object']).columns.tolist()\n",
    "# gender he international 不用onehot\n",
    "exclude_cols = ['international','gender','admission']\n",
    "\n",
    "categorical_cols = [col for col in categorical_cols if col not in exclude_cols]\n",
    "categorical_cols_ = [col for col in categorical_cols_ if col not in exclude_cols]\n",
    "print(f\"将对以下分类列进行one-hot编码: {categorical_cols}\")\n",
    "\n",
    "# 进行one-hot编码\n",
    "print(train_df.head(1))\n",
    "train_df_encoded = pd.get_dummies(train_df, columns=categorical_cols, drop_first=True)\n",
    "train_df_encoded['international'] = train_df['international'].map({True: 1, False: 0})#对international进行二进制编码\n",
    "train_df_encoded['gender'] = train_df['gender'].map({'Male': 1, 'Female': 0})#对gender进行标签编码\n",
    "train_df_encoded['admission'] = train_df['admission'].map({'Reject': 1, 'Waitlist': 0,'Admit':2})#对admission进行标签编码\n",
    "\n",
    "test_df_encoded = pd.get_dummies(test_df, columns=categorical_cols, drop_first=True)\n",
    "test_df_encoded['international'] = test_df['international'].map({True: 1, False: 0})#对international进行二进制编码\n",
    "test_df_encoded['gender'] = test_df['gender'].map({'Male': 1, 'Female': 0})#对gender进行标签编码\n",
    "test_df_encoded['admission'] = test_df['admission'].map({'Reject': 1, 'Waitlist': 0,'Admit':2})#对admission进行标签编码\n",
    "for col in train_df_encoded:\n",
    "    if col not in test_df_encoded.columns:\n",
    "        print(\"===\")\n",
    "        print(col)\n",
    "        test_df_encoded[col] = 0\n",
    "# 查看编码后的数据集形状\n",
    "print(f\"编码前数据集形状: {train_df.shape}\")\n",
    "print(f\"编码后数据集形状: {train_df_encoded.shape}\")\n",
    "print(f\"编码前数据集形状2: {test_df.shape}\")\n",
    "print(f\"编码后数据集形状2: {test_df_encoded.shape}\")\n",
    "\n",
    "# 查看前几行编码结果\n",
    "print(\"\\n编码后的数据集前5行:\")\n",
    "print(train_df_encoded.head(1))\n",
    "print(\"\\n编码后的数据集前5行:\")\n",
    "print(test_df_encoded.head(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11c19e0",
   "metadata": {},
   "source": [
    "对数值取值差异大的连续型gpa,gmat,work_exp进行归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f890ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 数据标准化\n",
    "train_df_encoded['gpa'] = (train_df['gpa'] - train_df['gpa'].mean()) / train_df['gpa'].std()\n",
    "train_df_encoded['gmat'] = (train_df['gmat'] - train_df['gmat'].mean()) / train_df['gmat'].std()\n",
    "train_df_encoded['work_exp'] = (train_df['work_exp'] - train_df['work_exp'].mean()) / train_df['work_exp'].std()\n",
    "\n",
    "# 数据标准化_test\n",
    "test_df_encoded['gpa'] = (test_df['gpa'] - test_df['gpa'].mean()) / test_df['gpa'].std()\n",
    "test_df_encoded['gmat'] = (test_df['gmat'] - test_df['gmat'].mean()) / test_df['gmat'].std()\n",
    "test_df_encoded['work_exp'] = (test_df['work_exp'] - test_df['work_exp'].mean()) / test_df['work_exp'].std()\n",
    "print(test_df_encoded.head(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b0069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "lr = 0.0001\n",
    "epoch = 50\n",
    "batch = 64\n",
    "# 4. 使用scikit-learn的MLP分类器\n",
    "print(\"\\n训练MLP分类器...\")\n",
    "#x_train = np.array(train_df.drop(columns=['admission']))\n",
    "\n",
    "# train_df_encoded['admission_status'] = 0  # 默认拒绝\n",
    "# train_df_encoded.loc[train_df_encoded['admission_Waitlist'] == True, 'admission_status'] = 1\n",
    "# train_df_encoded.loc[(train_df_encoded['admission_Reject'] == False) & (train_df_encoded['admission_Waitlist'] == False), 'admission_status'] = 2\n",
    "\n",
    "for col in train_df_encoded.columns:\n",
    "    if train_df_encoded[col].dtype == bool:\n",
    "        train_df_encoded[col] = train_df_encoded[col].map({True: 1, False: 0})\n",
    "\n",
    "for col in test_df_encoded.columns:\n",
    "    if test_df_encoded[col].dtype == bool:\n",
    "        test_df_encoded[col] = test_df_encoded[col].map({True: 1, False: 0})\n",
    "# 3. 选择特征列（排除ID、目标变量相关列）\n",
    "feature_cols = [col for col in train_df_encoded.columns if col not in [\n",
    "    'application_id','admission'\n",
    "]]\n",
    "feature_cols_ = [col for col in test_df_encoded.columns if col not in [\n",
    "    'application_id','admission'\n",
    "]]\n",
    "#print(train_df_encoded.columns)\n",
    "# 4. 转换为NumPy数组\n",
    "X_train = train_df_encoded[feature_cols].to_numpy()  # 特征数据\n",
    "y_train = train_df_encoded[['admission']].to_numpy()  # 目标变量\n",
    "\n",
    "X_test = test_df_encoded[feature_cols_].to_numpy()  # 特征数据\n",
    "y_test = test_df_encoded[['admission']].to_numpy()  # 目标变量\n",
    "print(\"特征数据形状:\", X_train.shape)\n",
    "print(\"目标变量形状:\", y_train.shape)\n",
    "print(\"\\n前5行特征数据:\")\n",
    "print(X_train[:5])\n",
    "print(\"\\n前5行目标变量:\")\n",
    "print(y_train[:5])\n",
    "# hidden_dim\n",
    "# [input_dim, 128], ReLU()\n",
    "# [128, 256], ReLU()\n",
    "# [256, output_dim]\n",
    "# 定义MLP模型，使用指定的隐藏层结构\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 256),  # 第一层128个神经元，第二层256个神经元\n",
    "    activation='relu',              # 使用ReLU激活函数，符合要求\n",
    "    solver='adam',                  # Adam优化器，适合大多数场景\n",
    "    alpha=0.001,                   # L2正则化参数\n",
    "    batch_size=batch,              # 自动决定批次大小\n",
    "    learning_rate='constant',       # 固定学习率\n",
    "    learning_rate_init=lr,       # 初始学习率\n",
    "    max_iter=epoch,                   # 最大迭代次数\n",
    "    shuffle=True,                   # 每次迭代前打乱数据\n",
    "    random_state=42,                # 随机种子，保证结果可复现\n",
    "    verbose=True,                   # 训练过程中打印日志\n",
    "    early_stopping=True,            # 启用早停策略\n",
    "    validation_fraction=0.1,        # 10%的训练数据作为验证集\n",
    "    tol=1e-4,                       # 早停的容差\n",
    "    n_iter_no_change=10             # 10轮迭代无改善则停止\n",
    ")\n",
    "X_train_for_torch = X_train\n",
    "y_train_for_torch = y_train\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# 5. 模型评估\n",
    "print(\"\\n模型评估:\")\n",
    "y_pred = mlp.predict(X_test)\n",
    "y_pred_proba = mlp.predict_proba(X_test)[:, 1]  # 正类的预测概率\n",
    "\n",
    "print(f\"测试集准确率: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\n分类报告:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "# print(\"\\n混淆矩阵:\")\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# 6. 绘制训练损失曲线\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mlp.loss_curve_)\n",
    "plt.title('MLP Training Loss Curve')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb444825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, dropout_rate=0.2):\n",
    "        # 在初始化模型之前添加\n",
    "        print(torch.cuda.is_available()) \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(self.device)\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 128)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.linear2 = nn.Linear(128, 256)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.linear3 = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = x.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.forward(x)\n",
    "            return torch.argmax(outputs, dim=1)\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None, epochs=100, lr=0.001, batch_size=32):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        \n",
    "        参数:\n",
    "        X_train: 训练特征，形状为 [样本数, 特征数]\n",
    "        y_train: 训练标签，形状为 [样本数]，每个元素是0,1,2之类的类别索引\n",
    "        \"\"\"\n",
    "        # 确保标签是长整型\n",
    "        if not isinstance(y_train, torch.Tensor):\n",
    "            y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "        elif y_train.dtype != torch.long:\n",
    "            y_train = y_train.long()\n",
    "        \n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # 创建数据加载器\n",
    "        dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            self.train()  # 设置为训练模式\n",
    "            \n",
    "            for batch_X, batch_y in dataloader:\n",
    "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.forward(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}')\n",
    "                \n",
    "                # 验证集评估\n",
    "                if X_val is not None and y_val is not None:\n",
    "                    val_accuracy = self.evaluate(X_val, y_val)\n",
    "                    print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"评估模型性能\"\"\"\n",
    "        # 在evaluate方法中：\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        self.eval()  # 设置为评估模式\n",
    "        with torch.no_grad():\n",
    "            # 预测结果需要计算指标时：\n",
    "            predictions = self.predict(X)\n",
    "            predictions = predictions.cpu().numpy()\n",
    "            y_true = y.cpu().numpy()\n",
    "            accuracy = accuracy_score(y_true, predictions)\n",
    "        return accuracy\n",
    "\n",
    "# 使用示例\n",
    "# 确保您的数据格式正确：\n",
    "# X_train_for_torch: 形状为 [样本数, 特征数] 的numpy数组或tensor\n",
    "# y_train_for_torch: 形状为 [样本数] 的类别索引数组 (如 [0, 1, 2, 0, 1, 2,...])\n",
    "\n",
    "\n",
    "\n",
    "X_train = torch.tensor(X_train_for_torch, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_for_torch, dtype=torch.long)\n",
    "y_train = y_train.squeeze()\n",
    "\n",
    "# cuda = torch.device('cuda')\n",
    "# X_train = X_train.to(cuda)\n",
    "# y_train = y_train.to(cuda)\n",
    "\n",
    "# 检查标签格式\n",
    "print(f\"标签形状: {y_train.shape}\")\n",
    "print(f\"标签唯一值: {torch.unique(y_train)}\")\n",
    "print(f\"标签范围: {y_train.min()} 到 {y_train.max()}\")\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(torch.unique(y_train))\n",
    "print(f\"输入维度: {input_dim}, 类别数: {num_classes}\")\n",
    "\n",
    "mlp = MLP(input_dim, num_classes)\n",
    "mlp = mlp.to(mlp.device)  # 将模型移动到GPU\n",
    "mlp.fit(X_train, y_train,epochs = epoch,lr = lr)\n",
    "\n",
    "# 预测\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_pred = mlp.predict(X_test)\n",
    "y_pred = y_pred.cpu().numpy()\n",
    "\n",
    "acr =0\n",
    "for i ,j in zip(y_test,y_pred):\n",
    "    if i == j:\n",
    "        acr+=1\n",
    "    else:\n",
    "        pass\n",
    "acr = acr/len(y_test)\n",
    "print(f'准确率为：{acr}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d987aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import matplotlib.font_manager as fm\n",
    "# from matplotlib.ticker import PercentFormatter\n",
    "# import warnings\n",
    "\n",
    "# # -------------------------- 1. 字体设置（彻底解决中文显示问题）--------------------------\n",
    "# # 强制指定支持完整中文的字体列表，按优先级排序\n",
    "# plt.rcParams[\"font.family\"] = [\"SimHei\", \"WenQuanYi Micro Hei\", \"Heiti TC\", \"Microsoft YaHei\", \"Arial Unicode MS\"]\n",
    "# plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "\n",
    "# # 验证字体是否有效\n",
    "# def verify_chinese_font():\n",
    "#     try:\n",
    "#         fig = plt.figure()\n",
    "#         ax = fig.add_subplot(111)\n",
    "#         ax.text(0.5, 0.5, '测试中文显示：录取、沃顿商学院、均值', \n",
    "#                 ha='center', va='center', fontsize=12)\n",
    "#         plt.close(fig)\n",
    "#         return True\n",
    "#     except:\n",
    "#         return False\n",
    "\n",
    "# # 如果字体验证失败，尝试更通用的配置\n",
    "# if not verify_chinese_font():\n",
    "#     plt.rcParams[\"font.family\"] = [\"Arial Unicode MS\", \"sans-serif\"]\n",
    "\n",
    "# plt.rcParams['font.size'] = 10\n",
    "# plt.rcParams['figure.dpi'] = 100\n",
    "# plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# # 定义沃顿风格配色\n",
    "# colors = ['#2E86AB', '#A23B72', '#F18F01']  # 沃顿蓝、紫色、橙色\n",
    "\n",
    "\n",
    "# # -------------------------- 2. 核心可视化函数（保持不变）--------------------------\n",
    "# def plot_wharton_admission_overview(df):\n",
    "#     \"\"\"1. 沃顿申请整体概览：录取率+关键结构特征\"\"\"\n",
    "#     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "#     # 子图1：录取状态分布\n",
    "#     admission_count = df['admission'].value_counts()\n",
    "#     admission_pct = df['admission'].value_counts(normalize=True) * 100\n",
    "    \n",
    "#     wedges, texts, autotexts = ax1.pie(\n",
    "#         admission_count.values, \n",
    "#         labels=admission_count.index,\n",
    "#         autopct='%1.1f%%',\n",
    "#         colors=colors,\n",
    "#         startangle=90,\n",
    "#         textprops={'fontsize': 11}\n",
    "#     )\n",
    "#     ax1.set_title('沃顿商学院2025届申请录取状态分布\\n（总申请数：{}）'.format(len(df)), fontsize=13, pad=20)\n",
    "    \n",
    "#     # 子图2：核心结构特征汇总\n",
    "#     gender_pct = df['gender'].value_counts(normalize=True) * 100\n",
    "#     international_pct = df['international'].value_counts(normalize=True) * 100\n",
    "#     major_pct = df['major'].value_counts(normalize=True) * 100\n",
    "    \n",
    "#     summary_text = f\"\"\"\n",
    "#     申请者核心结构：\n",
    "#     • 性别比例：男性 {gender_pct.get('男', 0):.1f}% / 女性 {gender_pct.get('女', 0):.1f}%\n",
    "#     • 国际学生占比：{international_pct.get('是', 0):.1f}%\n",
    "#     • 本科专业分布：\n",
    "#       - 商科 {major_pct.get('商科', 0):.1f}%\n",
    "#       - 理工科 {major_pct.get('理工科', 0):.1f}%\n",
    "#       - 人文学科 {major_pct.get('人文学科', 0):.1f}%\n",
    "#     \"\"\"\n",
    "#     ax2.text(0.1, 0.9, summary_text, transform=ax2.transAxes, \n",
    "#              fontsize=12, verticalalignment='top',\n",
    "#              bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "#     ax2.axis('off')\n",
    "#     ax2.set_title('申请者核心结构概览', fontsize=13, pad=20)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# def plot_academic_indicators(df):\n",
    "#     \"\"\"2. 学术硬指标分析：GPA/GMAT分布+与录取的关系\"\"\"\n",
    "#     fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "#     # 子图1：GPA整体分布\n",
    "#     sns.histplot(df['gpa'], kde=True, bins=20, color='#2E86AB', ax=ax1)\n",
    "#     ax1.axvline(df['gpa'].mean(), color='red', linestyle='--', linewidth=2, \n",
    "#                 label=f'均值：{df[\"gpa\"].mean():.2f}')\n",
    "#     ax1.set_title('申请者GPA分布（4.0分制）', fontsize=12)\n",
    "#     ax1.set_xlabel('GPA')\n",
    "#     ax1.set_ylabel('申请人数')\n",
    "#     ax1.legend()\n",
    "    \n",
    "#     # 子图2：GMAT整体分布\n",
    "#     sns.histplot(df['gmat'], kde=True, bins=20, color='#A23B72', ax=ax2)\n",
    "#     ax2.axvline(df['gmat'].mean(), color='red', linestyle='--', linewidth=2, \n",
    "#                 label=f'均值：{df[\"gmat\"].mean():.0f}')\n",
    "#     ax2.set_title('申请者GMAT分布（满分800）', fontsize=12)\n",
    "#     ax2.set_xlabel('GMAT分数')\n",
    "#     ax2.set_ylabel('申请人数')\n",
    "#     ax2.legend()\n",
    "    \n",
    "#     # 子图3：不同录取状态的GPA箱线图\n",
    "#     sns.boxplot(x='admission', y='gpa', data=df, hue='admission',\n",
    "#                 palette=colors, ax=ax3, legend=False)\n",
    "#     ax3.set_title('不同录取状态的GPA分布', fontsize=12)\n",
    "#     ax3.set_xlabel('录取状态')\n",
    "#     ax3.set_ylabel('GPA')\n",
    "#     # 添加各组均值标注\n",
    "#     for i, status in enumerate(df['admission'].unique()):\n",
    "#         mean_gpa = df[df['admission'] == status]['gpa'].mean()\n",
    "#         ax3.text(i, mean_gpa + 0.05, f'均值：{mean_gpa:.2f}', ha='center', fontsize=10)\n",
    "    \n",
    "#     # 子图4：不同录取状态的GMAT箱线图\n",
    "#     sns.boxplot(x='admission', y='gmat', data=df, hue='admission',\n",
    "#                 palette=colors, ax=ax4, legend=False)\n",
    "#     ax4.set_title('不同录取状态的GMAT分布', fontsize=12)\n",
    "#     ax4.set_xlabel('录取状态')\n",
    "#     ax4.set_ylabel('GMAT分数')\n",
    "#     # 添加各组均值标注\n",
    "#     for i, status in enumerate(df['admission'].unique()):\n",
    "#         mean_gmat = df[df['admission'] == status]['gmat'].mean()\n",
    "#         ax4.text(i, mean_gmat + 20, f'均值：{mean_gmat:.0f}', ha='center', fontsize=10)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# # -------------------------- 3. 其他函数和主程序（保持不变）--------------------------\n",
    "# def plot_work_background(df):\n",
    "#     \"\"\"3. 工作背景分析：行业分布+工作年限与录取的关系\"\"\"\n",
    "#     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    \n",
    "#     # 子图1：工作行业分布（Top8行业）\n",
    "#     work_industry_count = df['work_industry'].value_counts()\n",
    "#     top8_industries = work_industry_count.head(8)\n",
    "#     other_count = work_industry_count.iloc[8:].sum()\n",
    "#     if other_count > 0:\n",
    "#         top8_industries['其他'] = other_count\n",
    "    \n",
    "#     sns.barplot(x=top8_industries.values, y=top8_industries.index, color='#F18F01', ax=ax1)\n",
    "#     # 添加数量+占比标注\n",
    "#     total = len(df)\n",
    "#     for i, (industry, count) in enumerate(top8_industries.items()):\n",
    "#         pct = (count / total) * 100\n",
    "#         ax1.text(count + 5, i, f'{count}人（{pct:.1f}%）', va='center', fontsize=9)\n",
    "#     ax1.set_title('申请者工作行业分布（Top8）', fontsize=12)\n",
    "#     ax1.set_xlabel('申请人数')\n",
    "#     ax1.set_ylabel('工作行业')\n",
    "    \n",
    "#     # 子图2：工作年限与录取率的关系\n",
    "#     df['work_exp_group'] = pd.cut(\n",
    "#         df['work_exp'], \n",
    "#         bins=[0, 1, 3, 5, np.inf], \n",
    "#         labels=['0-1年', '1-3年', '3-5年', '5年以上']\n",
    "#     )\n",
    "#     # 计算每组的录取率\n",
    "#     work_exp_admission = df.groupby('work_exp_group')['admission'].apply(\n",
    "#         lambda x: (x == '录取').sum() / len(x) * 100\n",
    "#     ).reset_index()\n",
    "    \n",
    "#     sns.barplot(x='work_exp_group', y='admission', data=work_exp_admission, \n",
    "#                 hue='work_exp_group', color='#2E86AB', ax=ax2, legend=False)\n",
    "#     ax2.yaxis.set_major_formatter(PercentFormatter())\n",
    "#     # 添加录取率标注\n",
    "#     for i, rate in enumerate(work_exp_admission['admission']):\n",
    "#         ax2.text(i, rate + 1, f'{rate:.1f}%', ha='center', fontsize=10)\n",
    "#     ax2.set_title('不同工作年限的录取率', fontsize=12)\n",
    "#     ax2.set_xlabel('工作年限')\n",
    "#     ax2.set_ylabel('录取率（%）')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# def plot_key_correlations(df):\n",
    "#     \"\"\"4. 关键特征相关性：学术+工作特征热力图+国际学生录取差异\"\"\"\n",
    "#     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "#     # 子图1：数值特征相关性热力图\n",
    "#     numerical_cols = ['gpa', 'gmat', 'work_exp']\n",
    "#     corr_matrix = df[numerical_cols].corr()\n",
    "    \n",
    "#     sns.heatmap(\n",
    "#         corr_matrix, \n",
    "#         annot=True, \n",
    "#         fmt='.2f', \n",
    "#         cmap='coolwarm', \n",
    "#         square=True, \n",
    "#         cbar_kws={'shrink': 0.8},\n",
    "#         ax=ax1\n",
    "#     )\n",
    "#     ax1.set_title('学术+工作特征相关性热力图', fontsize=12)\n",
    "    \n",
    "#     # 子图2：国际学生vs本地学生的录取率对比\n",
    "#     international_admission = df.groupby('international')['admission'].apply(\n",
    "#         lambda x: (x == '录取').sum() / len(x) * 100\n",
    "#     ).reset_index()\n",
    "    \n",
    "#     sns.barplot(x='international', y='admission', data=international_admission, \n",
    "#                 hue='international', palette=['#A23B72', '#2E86AB'], ax=ax2, legend=False)\n",
    "#     ax2.yaxis.set_major_formatter(PercentFormatter())\n",
    "#     # 添加录取率标注\n",
    "#     for i, rate in enumerate(international_admission['admission']):\n",
    "#         ax2.text(i, rate + 1, f'{rate:.1f}%', ha='center', fontsize=10)\n",
    "#     ax2.set_title('国际学生vs本地学生录取率对比', fontsize=12)\n",
    "#     ax2.set_xlabel('是否国际学生')\n",
    "#     ax2.set_ylabel('录取率（%）')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# def plot_major_industry_admission(df):\n",
    "#     \"\"\"5. 本科专业与工作行业的录取率分析\"\"\"\n",
    "#     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    \n",
    "#     # 子图1：不同本科专业的录取率\n",
    "#     major_admission = df.groupby('major')['admission'].apply(\n",
    "#         lambda x: (x == '录取').sum() / len(x) * 100\n",
    "#     ).reset_index()\n",
    "    \n",
    "#     sns.barplot(x='major', y='admission', data=major_admission, \n",
    "#                 hue='major', palette=colors, ax=ax1, legend=False)\n",
    "#     ax1.yaxis.set_major_formatter(PercentFormatter())\n",
    "#     for i, rate in enumerate(major_admission['admission']):\n",
    "#         ax1.text(i, rate + 1, f'{rate:.1f}%', ha='center', fontsize=10)\n",
    "#     ax1.set_title('不同本科专业的录取率', fontsize=12)\n",
    "#     ax1.set_xlabel('本科专业')\n",
    "#     ax1.set_ylabel('录取率（%）')\n",
    "    \n",
    "#     # 子图2：Top5工作行业的录取率\n",
    "#     top5_industries = df['work_industry'].value_counts().head(5).index\n",
    "#     industry_admission = df[df['work_industry'].isin(top5_industries)].groupby('work_industry')['admission'].apply(\n",
    "#         lambda x: (x == '录取').sum() / len(x) * 100\n",
    "#     ).reset_index().sort_values('admission', ascending=False)\n",
    "    \n",
    "#     sns.barplot(x='admission', y='work_industry', data=industry_admission, \n",
    "#                 hue='work_industry', palette=colors, ax=ax2, legend=False)\n",
    "#     ax2.xaxis.set_major_formatter(PercentFormatter())\n",
    "#     for i, rate in enumerate(industry_admission['admission']):\n",
    "#         ax2.text(rate + 1, i, f'{rate:.1f}%', va='center', fontsize=10)\n",
    "#     ax2.set_title('Top5工作行业的录取率（从高到低）', fontsize=12)\n",
    "#     ax2.set_xlabel('录取率（%）')\n",
    "#     ax2.set_ylabel('工作行业')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# # 生成模拟数据\n",
    "# np.random.seed(42)\n",
    "# n_samples = 3200\n",
    "\n",
    "# train_df_encoded =train_df\n",
    "# # 执行分析\n",
    "# print(\"沃顿商学院2025届申请数据概况：\")\n",
    "# print(f\"总申请人数：{len(train_df_encoded)}人\")\n",
    "# print(f\"录取率：{(train_df_encoded['admission'] == '录取').mean():.2%}\")\n",
    "# print(f\"等待名单比例：{(train_df_encoded['admission'] == '等待名单').mean():.2%}\")\n",
    "# print(\"\\n开始可视化分析...\\n\")\n",
    "\n",
    "# plot_wharton_admission_overview(train_df_encoded)\n",
    "# plot_academic_indicators(train_df_encoded)\n",
    "# plot_work_background(train_df_encoded)\n",
    "# plot_key_correlations(train_df_encoded)\n",
    "# plot_major_industry_admission(train_df_encoded)\n",
    "\n",
    "# print(\"可视化分析完成！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200ad88b",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871194e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def linear_layer(x, w, b):\n",
    "    \"\"\"\n",
    "    线性层前向传播\n",
    "    x: 输入张量，形状为 (N, C, H, W)\n",
    "    w: 权重张量，形状为 (C, H, W)\n",
    "    b: 偏置张量，形状为 (1, 1, 1)\n",
    "    \"\"\"\n",
    "    return x @ w + b\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    对输入张量 x 执行元素级的 ReLU (Rectified Linear Unit) 操作。\n",
    "    公式为: f(x) = max(0, x)\n",
    "    \"\"\"\n",
    "    # ===== 在此实现 =====\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def flatten(x):\n",
    "    \"\"\"\n",
    "    将一个四维张量 (N, C, H, W) 展平为一个二维张量 (N, C*H*W)。\n",
    "    N 是批量大小，需要保持不变。\n",
    "    \"\"\"\n",
    "    # ===== 在此实现 =====\n",
    "    return x.reshape(x.shape[0], -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86964ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[-2], [-1], [0], [1], [2]])\n",
    "w1, b1 = np.array([[2]]),np.array([-1])\n",
    "w2, b2 = np.array([[-1]]),np.array([0.5])\n",
    "def forword_2_layer(x,use_relu = False):\n",
    "    \"\"\"\n",
    "    2层神经网络前向传播\n",
    "    x: 输入张量，形状为 (N, 1)\n",
    "    \"\"\"\n",
    "    z1 = linear_layer(x, w1, b1)\n",
    "    if use_relu:\n",
    "        a1 = relu(z1)\n",
    "    else:\n",
    "        a1 = z1\n",
    "    z2 = linear_layer(a1, w2, b2)\n",
    "    return z2\n",
    "res1 = forword_2_layer(x)\n",
    "res2 = forword_2_layer(x,use_relu = True)\n",
    "print(res1)\n",
    "print(res2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d68e1d",
   "metadata": {},
   "source": [
    "ReLU函数将负输入置为0，保持正输入不变，引入了非线性和稀疏性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72126365",
   "metadata": {},
   "source": [
    "使用ReLU函数可以引入非线性，帮助模型学习复杂的特征表示。使模型不仅能解决线性的问题,还可以解决更复杂的问题.根据万能近似定理,通过非线性激活函数的组合，神经网络可以近似任意复杂函数."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10397a4b",
   "metadata": {},
   "source": [
    "#### CNN背景"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41108850",
   "metadata": {},
   "source": [
    "假设输入一张 100x100 的单通道图，一个全连接层需要10000个权重才能仅仅让一个输出神经元连接到所有输入像素.作为对比，一个 3x3 的卷积核总共需要9个权重参数？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c61ae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def conv2d(x, w, b, stride=1, padding=0):\n",
    "    # 对输入进行填充\n",
    "    x = np.pad(x, ((0,0), (0,0), (padding,padding), (padding,padding)), \n",
    "               constant_values=0, mode='constant')\n",
    "    \n",
    "    # 获取输入和卷积核的维度信息\n",
    "    batch_size, in_channels, in_height, in_width = x.shape\n",
    "    out_channels, kernel_channels, kernel_size, kernel_size = w.shape\n",
    "    \n",
    "    # 确保输入通道数与卷积核通道数匹配\n",
    "    assert in_channels == kernel_channels, \"输入通道数与卷积核通道数不匹配\"\n",
    "    \n",
    "    # 先计算输出特征图的尺寸\n",
    "    out_height = (in_height - kernel_size) // stride + 1\n",
    "    out_width = (in_width - kernel_size) // stride + 1\n",
    "\n",
    "    # 初始化输出\n",
    "    y = np.zeros((batch_size, out_channels, out_height, out_width))\n",
    "\n",
    "    for i in range(batch_size):          # 遍历批次\n",
    "        for c_out in range(out_channels): # 遍历输出通道\n",
    "            for h in range(out_height):   # 遍历输出高度\n",
    "                for w_idx in range(out_width): # 遍历输出宽度\n",
    "                    # 计算窗口起始位置\n",
    "                    start_h = h * stride\n",
    "                    start_w = w_idx * stride\n",
    "                    \n",
    "                    # 提取滑动窗口区域\n",
    "                    window = x[i, :, start_h:start_h+kernel_size, \n",
    "                              start_w:start_w+kernel_size]\n",
    "                    #print(np.sum(window * w[c_out]) + b[c_out])\n",
    "                    y[i, c_out, h, w_idx] = np.sum((np.sum(window * w[c_out]) + b[c_out]))\n",
    "    \n",
    "    return y\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 阶段一: 验证特征检测\n",
    "    # 1. 定义一个 5x5 的图像，中心有一个“十字”图案\n",
    "    image_centered = np.array([[\n",
    "        [0, 0, 0, 0, 0],\n",
    "        [0, 0, 1, 0, 0],\n",
    "        [0, 1, 1, 1, 0],\n",
    "        [0, 0, 1, 0, 0],\n",
    "        [0, 0, 0, 0, 0]\n",
    "    ]], dtype=np.float32).reshape(1, 1, 5, 5)  # (batch, channels, height, width)\n",
    "    \n",
    "    # 2. 设计一个 3x3 的卷积核，检测十字图案\n",
    "    w = np.array([[[\n",
    "        [1, 1, 1],\n",
    "        [1, 1, 1],\n",
    "        [1, 1, 1]\n",
    "    ]]], dtype=np.float32).reshape(1, 1, 3, 3)  # (out_channels, in_channels, kernel_size, kernel_size)\n",
    "\n",
    "    b = np.array([0], dtype=np.float32)  # 偏置\n",
    "    \n",
    "    # 3. 执行卷积，观察输出\n",
    "    res = conv2d(image_centered, w, b, padding=1, stride=1)\n",
    "    print(\"阶段一输出（中心十字）:\")\n",
    "    #print(res.shape)\n",
    "    print(res[0, 0]) \n",
    "    \n",
    "    # 阶段二: 平移不变性\n",
    "    # 1. 创建一个新图像，将“十字”图案向右下方平移一格\n",
    "    image_shifted = np.array([[\n",
    "        [0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0],\n",
    "        [0, 0, 1, 1, 1],\n",
    "        [0, 0, 0, 1, 0]\n",
    "    ]], dtype=np.float32).reshape(1, 1, 5, 5)\n",
    "    \n",
    "    # 2. 使用完全相同的卷积核进行卷积，并观察输出\n",
    "    res_shifted = conv2d(image_shifted, w, b, padding=1, stride=1)\n",
    "    print(\"\\n阶段二输出（平移后十字）:\")\n",
    "    print(res_shifted[0, 0])  # 输出第一个样本、第一个通道的特征图\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dfb482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpmath import ker\n",
    "import numpy as np\n",
    "\n",
    "# 创建4x4特征图，左上角有一个2x2的高激活区域\n",
    "feature_map1 = [[11, 2, 1, 3],\n",
    "                [10, 3, 1, 1],\n",
    "                [1, 1, 2, 1],\n",
    "                [3, 1, 1, 1]]\n",
    "feature_map1 = np.array(feature_map1)\n",
    "# feature_map1[:2, :2] = 10  # 2x2高激活区域\n",
    "# feature_map1[feature_map1 == 0] = 1  # 其他区域设为低激活值\n",
    "\n",
    "\n",
    "feature_map2 = [[3, 11, 3, 1],\n",
    "                [1, 10, 3, 1],\n",
    "                [1, 1, 1, 2],\n",
    "                [1, 3, 1, 1]]\n",
    "feature_map2 = np.array(feature_map2)\n",
    "# feature_map2[1:3, 0:2] = 10  # 平移后的2x2高激活区域\n",
    "# feature_map2[feature_map2 == 0] = 1  # 其他区域设为低激活值\n",
    "\n",
    "feature_map1 = feature_map1.reshape(1, 1, 4, 4)\n",
    "feature_map2 = feature_map2.reshape(1, 1, 4, 4)\n",
    "\n",
    "print(\"第一个特征图（原始激活模式）:\")\n",
    "print(feature_map1)\n",
    "print(\"\\n第二个特征图（激活模式平移1个像素）:\")\n",
    "print(feature_map2)\n",
    "\n",
    "def max_pool2d(x, kernel_size=2, stride=2):\n",
    "    in_batch,in_channel,in_height,in_width = x.shape\n",
    "    out_width,out_height = (int)(in_width/kernel_size),(int)(in_height/kernel_size)\n",
    "    \n",
    "    y = np.zeros((in_batch,in_channel,out_height,out_width))\n",
    "    for i in range(in_batch):\n",
    "        for j in range(in_channel):\n",
    "            for k in range(out_height):\n",
    "                for l in range(out_width):\n",
    "                    y[i,j,k,l] = np.max(x[i,j,k*stride:k*stride+kernel_size,l*stride:l*stride+kernel_size])\n",
    "    return y\n",
    "res1 = max_pool2d(feature_map1)\n",
    "print(res1)\n",
    "res2 = max_pool2d(feature_map2)\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb378309",
   "metadata": {},
   "source": [
    "最大池化有“聚焦局部最大值，忽略位置细节”的特性,“平移不变性” 的核心定义是：当输入特征的位置发生微小移动时，输出特征仍能保留关键信息,由于实验现象,可以得出微小平移不改变 “关键激活信  息\"故证明了最大池化层能提供一定程度的“平移不变性”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565429fd",
   "metadata": {},
   "source": [
    "池化层的核心功能之一是主动降低特征图的空间维度 高 x 宽 ,提取其中关键的特征，从而从 “计算量”“内存占用”“过拟合风险” 三个维度提升网络效率\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67146e1",
   "metadata": {},
   "source": [
    "如果换成平均池化,输出的特征图会有很明显的平移变化,平均池化对平移更敏感，且弱化高激活特,导致高激活特征的对比度大幅下降.\n",
    "\n",
    "最大池化\t突出局部关键特征 适合需要强化显著特征的场景\n",
    "\n",
    "平均池化\t保留局部全局信息 适合需要平滑输出、保留整体趋势的场景"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd826990",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93582505",
   "metadata": {},
   "source": [
    "softmax 函数的输出向量具备以下数学特性：\n",
    "1. 每个元素都在 0 到 1 之间，且所有元素的和为 1。\n",
    "2. 输出向量的每个元素都可以被理解为类别的概率。\n",
    "3. 输出向量的元素之间是互斥的，即一个元素的高值会抑制其他元素的高值。\n",
    "4. 扩大了高值元素和低值元素的差距，使得高值元素的概率差异更大。\n",
    "故而使得softmax函数在分类中可以扩大高值元素的概率差异，从而提高分类的准确性。并可以直接计算出各个分类类型的概率."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f71e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(x,axis):\n",
    "    \"\"\"\n",
    "    计算softmax函数\n",
    "    :param x: 输入向量\n",
    "    :return: 输出向量\n",
    "    \"\"\"\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740d2750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "import struct\n",
    "from array import array\n",
    "\n",
    "# (在此之前应有已实现的 conv2d, relu, max_pool2d, flatten,linear_layer函数)\n",
    "# 固定随机种子，保证权重初始化一致\n",
    "np.random.seed(114514)\n",
    "\n",
    "def softmax(x):\n",
    "    #axis = logits.ndim - 1\n",
    "    \"\"\"\n",
    "    实现Softmax函数。\n",
    "    \"\"\"\n",
    "    # ===== 在此实现 =====\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# --- MNIST 数据集读取函数 ---\n",
    "def read_images(filename):\n",
    "    \"\"\"\n",
    "    读取MNIST图像文件\n",
    "    参数:\n",
    "      filename: MNIST图像文件路径\n",
    "    返回:\n",
    "      images: 图像数组列表\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as file:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "        if magic != 2051:\n",
    "            raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "        \n",
    "        image_data = array(\"B\", file.read())\n",
    "        \n",
    "    images = []\n",
    "    for i in range(size):\n",
    "        img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "        img = img.reshape(rows, cols)\n",
    "        images.append(img)\n",
    "    \n",
    "    return images\n",
    "\n",
    "# =========================================================\n",
    "# ===== 任务：请根据下面的规约，在此处实现 TinyCNN_for_MNIST 类 =====\n",
    "# =========================================================\n",
    "#\n",
    "# --- 模型架构规约 ---\n",
    "# 1. 构造函数 `__init__(self)`:\n",
    "# 架构固定：Conv(1->4, k=3, stride=1, pad=1) -> ReLU -> MaxPool(2x2, s=2) -> Flatten -> Linear(->10 类)\n",
    "# 2. 前向传播方法 `forward(self, x)`:\n",
    "#    - 接收一个形状为 (N, 1, 28, 28) 的张量 x。\n",
    "#    - 按照以下顺序依次调用你实现的算子：\n",
    "#      Conv2d -> ReLU -> MaxPool2d -> Flatten -> Linear -> Softmax\n",
    "#    - 返回最终的 logits (Linear层输出) 和 probs (Softmax层输出)。\n",
    "def Linear(x, output):\n",
    "    # 修正权重维度：(in_features, out_features) → 确保 x·w 维度匹配\n",
    "    w = np.random.randn(x.shape[1], output)\n",
    "    b = np.zeros(output)\n",
    "    return np.dot(x, w) + b\n",
    "class TinyCNN_for_MNIST:\n",
    "# ===== 在此实现你的类 =====\n",
    "    def __init__(self):\n",
    "        #self.conv1 = self.conv_(x, 4, 3, 1, 1)\n",
    "        #self.relu1 = relu()\n",
    "        #self.maxpool1 = max_pool2d(2, 2)\n",
    "        #self.flatten = flatten()\n",
    "        #self.linear1 = Linear()\n",
    "        #self.softmax = Softmax()\n",
    "        pass\n",
    "    def conv_ (self,x, out_, k, s, p):\n",
    "        res=np.zeros((out_,x.shape[0],x.shape[2],x.shape[3]))\n",
    "        for i in range(out_):\n",
    "            w = np.random.randn(x.shape[1], k, k).reshape(1, x.shape[1], k, k)\n",
    "            b = np.zeros(out_).reshape(1, out_, 1, 1)\n",
    "            res[i] = conv2d(x, w, b, s, p)\n",
    "        return res\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv_(x, 4, 3, 1, 1)\n",
    "        x = relu(x)\n",
    "        x = max_pool2d(x,2,2)\n",
    "        x = flatten(x)\n",
    "        x = Linear(x,10)\n",
    "        #print(x)\n",
    "        x2 = softmax(x)\n",
    "        return x,x2\n",
    "    \n",
    "\n",
    "# --- 测试脚本 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 设置 MNIST 测试集文件路径\n",
    "    # !! 请将此路径修改为你自己的文件路径\n",
    "    mnist_test_file = \"E:\\\\download\\\\AI派2025招新第一轮测试\\\\AI派2025招新第一轮测试\\\\mnist\\\\train-images.idx3-ubyte\"\n",
    "\n",
    "    if not os.path.exists(mnist_test_file):\n",
    "        print(f\"错误：找不到 MNIST 测试集文件 '{mnist_test_file}'\")\n",
    "    else:\n",
    "        # 2. 加载所有测试图像\n",
    "        test_images = read_images(mnist_test_file)\n",
    "        # 3. 选取第一张图像作为测试输入\n",
    "        first_test_image = test_images[0]\n",
    "        # 4. 预处理图像\n",
    "        input_tensor = (first_test_image.astype(np.float32) / 255.0 - 0.5) * 2.0\n",
    "        input_tensor = np.expand_dims(input_tensor, axis=(0, 1))\n",
    "        # 5. 实例化模型并执行前向传播\n",
    "        model = TinyCNN_for_MNIST()\n",
    "        logits, probs = model.forward(input_tensor)\n",
    "\n",
    "        print(\"Input Tensor Shape:\", input_tensor.shape)\n",
    "        print(\"Logits shape:\", logits.shape, \"Probs shape:\", probs.shape)\n",
    "        np.set_printoptions(precision=8, suppress=False)\n",
    "        print(\"\\nLogits:\", logits[0])\n",
    "        print(\"Probs:\", probs[0])\n",
    "        print(\"\\nChecksum logits sum:\", float(np.sum(logits)))\n",
    "        print(\"Checksum probs sum:\", float(np.sum(probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f62ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import gzip\n",
    "# import os\n",
    "# import struct\n",
    "# from array import array\n",
    "\n",
    "# # 网络层实现（保持不变）\n",
    "# class Conv2d:\n",
    "#     def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "#         self.in_channels = in_channels\n",
    "#         self.out_channels = out_channels\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.stride = stride\n",
    "#         self.padding = padding\n",
    "#         self.weights = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * np.sqrt(2.0 / (in_channels * kernel_size * kernel_size))\n",
    "#         self.bias = np.zeros(out_channels)\n",
    "    \n",
    "#     def __call__(self, x):\n",
    "#         N, C_in, H, W = x.shape\n",
    "#         K, S, P = self.kernel_size, self.stride, self.padding\n",
    "#         H_out = (H + 2*P - K) // S + 1\n",
    "#         W_out = (W + 2*P - K) // S + 1\n",
    "#         x_padded = np.pad(x, ((0,0), (0,0), (P,P), (P,P)), mode='constant')\n",
    "#         output = np.zeros((N, self.out_channels, H_out, W_out))\n",
    "        \n",
    "#         for n in range(N):\n",
    "#             for c_out in range(self.out_channels):\n",
    "#                 for h in range(H_out):\n",
    "#                     for w in range(W_out):\n",
    "#                         h_start, h_end = h * S, h * S + K\n",
    "#                         w_start, w_end = w * S, w * S + K\n",
    "#                         window = x_padded[n, :, h_start:h_end, w_start:w_end]\n",
    "#                         output[n, c_out, h, w] = np.sum(window * self.weights[c_out]) + self.bias[c_out]\n",
    "#         return output\n",
    "\n",
    "# class ReLU:\n",
    "#     def __call__(self, x):\n",
    "#         return np.maximum(0, x)\n",
    "\n",
    "# class MaxPool2d:\n",
    "#     def __init__(self, kernel_size, stride):\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.stride = stride\n",
    "    \n",
    "#     def __call__(self, x):\n",
    "#         N, C, H, W = x.shape\n",
    "#         K, S = self.kernel_size, self.stride\n",
    "#         H_out = (H - K) // S + 1\n",
    "#         W_out = (W - K) // S + 1\n",
    "#         output = np.zeros((N, C, H_out, W_out))\n",
    "        \n",
    "#         for n in range(N):\n",
    "#             for c in range(C):\n",
    "#                 for h in range(H_out):\n",
    "#                     for w in range(W_out):\n",
    "#                         h_start, h_end = h * S, h * S + K\n",
    "#                         w_start, w_end = w * S, w * S + K\n",
    "#                         output[n, c, h, w] = np.max(x[n, c, h_start:h_end, w_start:w_end])\n",
    "#         return output\n",
    "\n",
    "# class Flatten:\n",
    "#     def __call__(self, x):\n",
    "#         return x.reshape(x.shape[0], -1)\n",
    "\n",
    "# class Linear:\n",
    "#     def __init__(self, in_features, out_features):\n",
    "#         self.weights = np.random.randn(out_features, in_features) * np.sqrt(1.0 / in_features)\n",
    "#         self.bias = np.zeros(out_features)\n",
    "    \n",
    "#     def __call__(self, x):\n",
    "#         return np.dot(x, self.weights.T) + self.bias\n",
    "\n",
    "# # Softmax函数\n",
    "# np.random.seed(114514)\n",
    "# def softmax(logits):\n",
    "#     max_val = np.max(logits, axis=1, keepdims=True)\n",
    "#     exp_x = np.exp(logits - max_val)\n",
    "#     return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# ## ---------------------- 修复后的MNIST读取函数（自动适配压缩/非压缩） ----------------------\n",
    "# def read_images(filename):\n",
    "#     \"\"\"\n",
    "#     读取MNIST图像文件（自动适配.gz压缩和非压缩格式）\n",
    "#     参数：filename - 图像文件路径（如 train-images-idx3-ubyte 或 train-images-idx3-ubyte.gz）\n",
    "#     返回：images - 图像列表，每个元素是(28,28)的numpy数组\n",
    "#     \"\"\"\n",
    "#     # 关键：根据文件名是否以.gz结尾，选择打开方式\n",
    "#     if filename.endswith('.gz'):\n",
    "#         # 压缩文件：用gzip.open读取\n",
    "#         import gzip\n",
    "#         open_func = gzip.open\n",
    "#     else:\n",
    "#         # 非压缩文件：用普通open读取\n",
    "#         open_func = open\n",
    "    \n",
    "#     # 用选择的方式打开文件\n",
    "#     with open_func(filename, 'rb') as file:\n",
    "#         # 读取MNIST文件头（前16字节：魔术数、图像数、高、宽）\n",
    "#         magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "        \n",
    "#         # 校验文件类型（避免误读标签文件）\n",
    "#         if magic == 2049:\n",
    "#             raise ValueError(\"错误：你读取的是MNIST标签文件（魔术数2049）！请替换为图像文件（魔术数2051），图像文件名含'images'（如train-images-idx3-ubyte）\")\n",
    "#         if magic != 2051:\n",
    "#             raise ValueError(f\"错误：未知文件类型！魔术数为{magic}，MNIST图像文件魔术数应为2051\")\n",
    "        \n",
    "#         # 读取所有像素数据（无符号字节类型，范围0-255）\n",
    "#         from array import array\n",
    "#         image_data = array(\"B\", file.read())\n",
    "    \n",
    "#     # 将字节数据转换为(28,28)的numpy数组列表\n",
    "#     images = []\n",
    "#     for i in range(size):\n",
    "#         # 提取第i张图像的字节（每张图有rows*cols=784个字节）\n",
    "#         img_flat = image_data[i * rows * cols : (i+1) * rows * cols]\n",
    "#         # 重塑为(28,28)的二维数组\n",
    "#         img = np.array(img_flat).reshape(rows, cols)\n",
    "#         images.append(img)\n",
    "    \n",
    "#     return images\n",
    "    \n",
    "\n",
    "# # 模型类\n",
    "# class TinyCNN_for_MNIST:\n",
    "#     def __init__(self):\n",
    "#         self.conv1 = Conv2d(1, 4, 3, 1, 1)\n",
    "#         self.relu1 = ReLU()\n",
    "#         self.maxpool1 = MaxPool2d(2, 2)\n",
    "#         self.flatten = Flatten()\n",
    "#         self.linear1 = Linear(4*14*14, 10)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.relu1(x)\n",
    "#         x = self.maxpool1(x)\n",
    "#         x = self.flatten(x)\n",
    "#         logits = self.linear1(x)\n",
    "#         probs = softmax(logits)\n",
    "#         return logits, probs\n",
    "\n",
    "# # 测试脚本\n",
    "# if __name__ == \"__main__\":\n",
    "#     # 确保使用正确的图像文件，通常图像文件名为：\n",
    "#     # 训练集：train-images-idx3-ubyte.gz\n",
    "#     # 测试集：t10k-images-idx3-ubyte.gz\n",
    "#     mnist_test_file = \"E:\\\\download\\\\AI派2025招新第一轮测试\\\\AI派2025招新第一轮测试\\\\mnist\\\\train-images.idx3-ubyte\"\n",
    "\n",
    "#     if not os.path.exists(mnist_test_file):\n",
    "#         print(f\"错误：找不到文件 '{mnist_test_file}'\")\n",
    "#         print(\"请检查路径是否正确，确保文件名包含 'images' 而非 'labels'\")\n",
    "#     else:\n",
    "#         try:\n",
    "#             test_images = read_images(mnist_test_file)\n",
    "#             print(f\"成功读取 {len(test_images)} 张图像\")\n",
    "            \n",
    "#             first_test_image = test_images[0]\n",
    "#             input_tensor = (first_test_image.astype(np.float32) / 255.0 - 0.5) * 2.0\n",
    "#             input_tensor = np.expand_dims(input_tensor, axis=(0, 1))\n",
    "            \n",
    "#             model = TinyCNN_for_MNIST()\n",
    "#             logits, probs = model.forward(input_tensor)\n",
    "\n",
    "#             print(\"Input Tensor Shape:\", input_tensor.shape)\n",
    "#             print(\"Logits shape:\", logits.shape, \"Probs shape:\", probs.shape)\n",
    "#             print(\"\\nLogits:\", logits[0])\n",
    "#             print(\"Probs:\", probs[0])\n",
    "#             print(\"\\nProbs sum:\", float(np.sum(probs)))  # 应为1.0左右\n",
    "#         except ValueError as e:\n",
    "#             print(f\"读取失败：{e}\")\n",
    "#             print(\"请确认使用的是图像文件，MNIST文件命名规则：\")\n",
    "#             print(\"- 图像文件：*images-idx3-ubyte.gz\")\n",
    "#             print(\"- 标签文件：*labels-idx1-ubyte.gz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a433a",
   "metadata": {},
   "source": [
    "### 注意力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40920316",
   "metadata": {},
   "source": [
    "词嵌入是将文本中的词语或子词转换为低维稠密向量的技术,将每个词语映射到一个连续的向量空间中,使得词语之间的语义关系可以通过向量之间的距离来表示。\n",
    "作用在于让计算机能够理解词语的语义，并用数学方式表示词语之间的关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d69eb0e",
   "metadata": {},
   "source": [
    "传统的词表示方法容易出现维度爆炸,即词数过多导致维度过大,和语义缺失的问题,例如one_hot编码,词向量之间点积为零,无法数学方法表达词义之间的距离\n",
    "\n",
    "词嵌入的局限性有 \n",
    "- 容易陷入一词多意的困境和领域偏移的问题, \n",
    "- 且对大量的优质训练数据有依赖, \n",
    "- 缺乏可解释性等等"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5ab50b",
   "metadata": {},
   "source": [
    "一种常见的词嵌入模型是 Word2Vec 由 Google 在 2013 年提出,Word2Vec 的核心思想是 “上下文相似的词，其嵌入向量嵌入也相似\n",
    "- 低维稠密表示\n",
    "相比 One-Hot 编码，Word2Vec 的向量维度固定 50~300 维，极大降低了计算成本，同时保留语义信息。\n",
    "- 捕捉语义和语法关联\n",
    "语义上：“猫” 和 “狗” 的向量距离近，“医生” 和 “医院” 距离近；\n",
    "语法上：“run” 和 “ran”、“big” 和 “bigger”的向量关系相似。\n",
    "- 训练高效\n",
    "引入 “负采样” 即 Negative Sampling 和 “层次 Softmax” 技术，避免传统神经网络对所有词汇的概率计算，使训练速度提升 10~100 倍，可处理数十亿级文本。\n",
    "- 静态词向量\n",
    "每个词对应唯一向量,如 “苹果” 在 “吃苹果” 和 “苹果公司” 中向量相同,无法处理一词多义.这是其主要局限。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454a4280",
   "metadata": {},
   "source": [
    "多头多头自注意力核心是通过多个并行 “注意力头”，从不同视角学习序列内关联，再融合结果提升表达能力。每个头聚焦不同关联类型（如语法、语义），最后拼接并线性映射输出。\n",
    "缩放点积注意力公式：Attention (Q,K,V) = Softmax ((QKᵀ)/√dₖ) V 。\n",
    "Q（查询）：需关注的对象；K（键）：供关注的特征，Q 与 K 的点积算相似度；V（值）：实际需加权的信息。\n",
    "√dₖ（缩放因子）：避免 dₖ过大导致点积结果过大，使 Softmax 梯度稳定。\n",
    "Softmax：将相似度归一化为权重，最终输出是 V 的加权和。\n",
    "带掩码时加 Mask 项，屏蔽无效信息（如 padding 或未来内容）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cde21d2",
   "metadata": {},
   "source": [
    "#### 缩放点积注意力（Scaled Dot-Product Attention）计算公式\n",
    "\n",
    "#### 形式\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{Softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "\n",
    "### 参数含义说明\n",
    "\n",
    "| 参数符号         | 含义解释                                                                 |\n",
    "|------------------|--------------------------------------------------------------------------|\n",
    "| $Q$              | 查询向量（Query），代表当前需要关注的对象，用于计算与其他元素的关联强度   |\n",
    "| $K$              | 键向量（Key），代表可供关注的特征，与$Q$的点积结果反映两者关联程度       |\n",
    "| $V$              | 值向量（Value），代表实际需加权的信息，最终输出是$V$基于注意力权重的加权和 |\n",
    "| $QK^T$           | $Q$与$K$的转置矩阵相乘，得到原始关联度矩阵，元素$(i,j)$表示第$i$个查询与第$j$个键的关联强度 |\n",
    "| $\\sqrt{d_k}$     | 缩放因子，$d_k$为$Q$和$K$的维度，避免维度过大导致点积结果过大，缓解Softmax梯度消失 |\n",
    "| $\\text{Softmax}$ | 归一化函数，将关联度转换为注意力权重，使每个查询对应的权重之和为1         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23288f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from numpy import softmax\n",
    "np.random.seed(114514)\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = np.matmul(Q, K.transpose(0, 1, 3, 2)) / np.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores += mask * -1e9\n",
    "    attention_weights = softmax(scores)\n",
    "    output = np.matmul(attention_weights, V)\n",
    "    return output, attention_weights\n",
    "\n",
    "def multi_head_attention(embed_size, num_heads, input, mask=None):\n",
    "    head_size = embed_size // num_heads\n",
    "    wQ = np.random.randn(batch_size, seq_len, num_heads, head_size)\n",
    "    wK = np.random.randn(batch_size, seq_len, num_heads, head_size)\n",
    "    wV = np.random.randn(batch_size, seq_len, num_heads, head_size)\n",
    "    wO = np.random.randn(batch_size, seq_len, num_heads, head_size)\n",
    "    output, attention_weights = scaled_dot_product_attention(wQ, wK, wV, mask)\n",
    "    return output, attention_weights\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 10\n",
    "    seq_len = 20\n",
    "    embed_size = 128\n",
    "    num_heads = 8\n",
    "    input = np.random.randn(batch_size, seq_len, embed_size) \n",
    "    output, weights = multi_head_attention(embed_size, num_heads, input)\n",
    "\t\n",
    "    print(output.shape, weights.shape)\n",
    "    print(output[0][0][:10], weights[0][0][0][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f432a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"基于PyTorch的多头自注意力实现\"\"\"\n",
    "    def __init__(self, embed_size, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_size % num_heads == 0, \"嵌入维度(embed_size)必须能被头数(num_heads)整除\"\n",
    "\n",
    "        self.embed_size = embed_size  # 总嵌入维度\n",
    "        self.num_heads = num_heads    # 注意力头数量\n",
    "        self.head_size = embed_size // num_heads  # 单个头的维度\n",
    "\n",
    "        self.w_q = nn.Linear(embed_size, embed_size)\n",
    "        self.w_k = nn.Linear(embed_size, embed_size)\n",
    "        self.w_v = nn.Linear(embed_size, embed_size)\n",
    "        self.w_o = nn.Linear(embed_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        d_k = Q.size(-1)  # 单个头的维度（用于缩放）\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1))  # 形状 batch, heads, seq_q, seq_k\n",
    "        scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32, device=scores.device))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)  \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        output = torch.matmul(attn_weights, V)  \n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)  # 获取批次大小\n",
    "\n",
    "        Q = self.w_q(query) \n",
    "        K = self.w_k(key)   \n",
    "        V = self.w_v(value) \n",
    "        \n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)\n",
    "\n",
    "        attn_output, attn_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        attn_output =attn_output.transpose(1, 2).contiguous() \n",
    "        attn_output =attn_output.view(batch_size, -1, self.embed_size)  # 拼接为embed_size\n",
    "\n",
    "        final_output = self.w_o(attn_output)\n",
    "        \n",
    "        return final_output, attn_weights\n",
    "\n",
    "# 测试代码\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 构造测试输入（与任务2保持一致的形状）\n",
    "    batch_size = 10\n",
    "    seq_len = 20\n",
    "    embed_size = 128\n",
    "    num_heads = 8\n",
    "    input_tensor = torch.randn(batch_size, seq_len, embed_size)  # 随机输入张量\n",
    "\n",
    "    model = MultiHeadAttention(embed_size, num_heads)\n",
    "\n",
    "    output, attn_weights = model(input_tensor, input_tensor, input_tensor)\n",
    "\n",
    "    print(f\"输出张量形状: {output.shape}\")  \n",
    "    print(f\"注意力权重形状: {attn_weights.shape}\") \n",
    "    print(\"\\n输出张量[0,0,:10]（第1个样本、第1个位置的前10个维度）:\")\n",
    "    print(output[0][0][:10])\n",
    "    print(\"\\n注意力权重[0,0,0,:10]（第1个样本、第1个头部、第1个位置的前10个权重）:\")\n",
    "    print(attn_weights[0][0][0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4881de4",
   "metadata": {},
   "source": [
    "### 库调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf66e8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import requests\n",
    "from PIL import Image\n",
    "# load pipeline\n",
    "ckpt = \"google/siglip2-base-patch16-224\"\n",
    "image_classifier = pipeline(model=ckpt, task=\"zero-shot-image-classification\")\n",
    "\n",
    "# load image and candidate labels\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "candidate_labels = [\"2 cats\", \"a plane\", \"a remote\"]\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "# run inference\n",
    "outputs = image_classifier(image, candidate_labels)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3738811",
   "metadata": {},
   "source": [
    "#### 结果\n",
    "![结果](res.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd864dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# 1. 配置参数\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"  # 国内加速\n",
    "MODEL_CHECKPOINT = \"google/siglip2-base-patch16-224\"\n",
    "DATASET = \"ethz/food101\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32  # 根据显存调整\n",
    "TOP_K = 5  # 计算top-5准确率\n",
    "\n",
    "# 2. 加载数据集（只取验证集，每类前10张）\n",
    "dataset = load_dataset(DATASET, split=\"validation\")\n",
    "\n",
    "# 获取标签ID到文本的映射（从数据集README提取的101类食物名称）\n",
    "# 完整标签列表见：https://huggingface.co/datasets/ethz/food101#labels\n",
    "food_labels = [\n",
    "    \"apple pie\", \"baby back ribs\", \"baklava\", \"beef carpaccio\", \"beef tartare\",\n",
    "    \"beet salad\", \"beignets\", \"bibimbap\", \"bread pudding\", \"breakfast burrito\",\n",
    "    \"bruschetta\", \"caesar salad\", \"cannoli\", \"caprese salad\", \"carrot cake\",\n",
    "    \"ceviche\", \"cheesecake\", \"cheese plate\", \"chicken curry\", \"chicken quesadilla\",\n",
    "    \"chicken wings\", \"chocolate cake\", \"chocolate mousse\", \"churros\", \"clam chowder\",\n",
    "    \"club sandwich\", \"crab cakes\", \"creme brulee\", \"croque madame\", \"cup cakes\",\n",
    "    \"deviled eggs\", \"donuts\", \"dumplings\", \"edamame\", \"eggs benedict\", \"escargots\",\n",
    "    \"falafel\", \"filet mignon\", \"fish and chips\", \"foie gras\", \"french fries\",\n",
    "    \"french onion soup\", \"french toast\", \"fried calamari\", \"fried rice\", \"frozen yogurt\",\n",
    "    \"garlic bread\", \"gnocchi\", \"greek salad\", \"grilled cheese sandwich\", \"grilled salmon\",\n",
    "    \"guacamole\", \"gyoza\", \"hamburger\", \"hot and sour soup\", \"hot dog\", \"huevos rancheros\",\n",
    "    \"hummus\", \"ice cream\", \"lasagna\", \"lobster bisque\", \"lobster roll sandwich\",\n",
    "    \"macaroni and cheese\", \"macarons\", \"miso soup\", \"mussels\", \"nachos\", \"omelette\",\n",
    "    \"onion rings\", \"oysters\", \"pad thai\", \"paella\", \"pancakes\", \"panna cotta\", \"peking duck\",\n",
    "    \"pho\", \"pizza\", \"pork chop\", \"poutine\", \"prime rib\", \"pulled pork sandwich\", \"ramen\",\n",
    "    \"ravioli\", \"red velvet cake\", \"risotto\", \"samosa\", \"sashimi\", \"scallops\", \"seaweed salad\",\n",
    "    \"shrimp and grits\", \"spaghetti bolognese\", \"spaghetti carbonara\", \"spring rolls\",\n",
    "    \"steak\", \"strawberry shortcake\", \"sushi\", \"tacos\", \"takoyaki\", \"tiramisu\", \"tuna tartare\",\n",
    "    \"waffles\"\n",
    "]\n",
    "\n",
    "# 筛选每类前10张图（共101×10=1010张）\n",
    "def filter_per_class(example, class_id, count_per_class=10):\n",
    "    \"\"\"保留每个类别的前count_per_class张样本\"\"\"\n",
    "    if example[\"label\"] not in class_id:\n",
    "        class_id[example[\"label\"]] = 0\n",
    "    if class_id[example[\"label\"]] < count_per_class:\n",
    "        class_id[example[\"label\"]] += 1\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "class_counter = {}\n",
    "filtered_dataset = dataset.filter(\n",
    "    filter_per_class,\n",
    "    fn_kwargs={\"class_id\": class_counter, \"count_per_class\": 10}\n",
    ")\n",
    "print(f\"筛选后样本数: {len(filtered_dataset)}\")  # 应输出1010\n",
    "\n",
    "\n",
    "# 3. 加载模型和处理器\n",
    "processor = AutoProcessor.from_pretrained(MODEL_CHECKPOINT)\n",
    "model = AutoModel.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()  # 推理模式\n",
    "\n",
    "\n",
    "# 4. 批量推理并计算top-5准确率\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# 生成候选文本（统一格式为\"a photo of {food}\"，符合SigLIP训练习惯）\n",
    "candidate_texts = [f\"a photo of {label}\" for label in food_labels]\n",
    "\n",
    "# 按批次处理\n",
    "for i in range(0, len(filtered_dataset), BATCH_SIZE):\n",
    "    batch = filtered_dataset[i:i+BATCH_SIZE]\n",
    "    images = [Image.fromarray(img).convert(\"RGB\") for img in batch[\"image\"]]\n",
    "    true_labels = batch[\"label\"]  # 真实标签ID（0-100）\n",
    "    \n",
    "    # 预处理\n",
    "    inputs = processor(\n",
    "        text=candidate_texts,\n",
    "        images=images,\n",
    "        padding=\"max_length\",\n",
    "        max_length=64,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE, dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32)\n",
    "    \n",
    "    # 推理（关闭梯度计算）\n",
    "    with torch.no_grad():\n",
    "        outputs = model(** inputs)\n",
    "    \n",
    "    # 解析结果：获取每个样本的top-5预测标签ID\n",
    "    logits = outputs.logits_per_image  # 形状：[batch_size, 101]\n",
    "    top5_preds = torch.topk(logits, k=TOP_K, dim=1).indices.cpu().numpy()  # [batch_size, 5]\n",
    "    \n",
    "    # 统计正确数（真实标签是否在top-5中）\n",
    "    for pred_ids, true_id in zip(top5_preds, true_labels):\n",
    "        if true_id in pred_ids:\n",
    "            correct += 1\n",
    "    total += len(batch)\n",
    "    \n",
    "    # 打印进度\n",
    "    if (i // BATCH_SIZE) % 5 == 0:\n",
    "        print(f\"已处理 {i+len(batch)}/{len(filtered_dataset)} 样本，当前准确率: {correct/total:.4f}\")\n",
    "\n",
    "# 输出最终结果\n",
    "print(f\"\\nTop-{TOP_K} 准确率: {correct/total:.4f} ({correct}/{total})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666a3396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "import torch\n",
    "import umap  # 修改导入方式\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = [\"SimHei\", \"WenQuanYi Micro Hei\", \"Heiti TC\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False  # 正确显示负号\n",
    "\n",
    "# 定义要处理的类别和数量\n",
    "CLASSES = [\"pizza\", \"sushi\", \"hamburger\", \"ice_cream\", \"dumplings\"]\n",
    "NUM_PER_CLASS = 100\n",
    "\n",
    "# 加载Food101数据集（只加载训练集）\n",
    "print(\"加载Food101数据集...\")\n",
    "dataset = load_dataset(\"food101\", split=\"train\")\n",
    "\n",
    "# 筛选出我们需要的类别和样本\n",
    "filtered_samples = []\n",
    "labels = []\n",
    "\n",
    "print(\"筛选数据集中的样本...\")\n",
    "for cls in CLASSES:\n",
    "    # 找到该类别的所有样本\n",
    "    class_samples = [sample for sample in dataset if sample[\"label\"] == dataset.features[\"label\"].str2int(cls)]\n",
    "    # 只取前NUM_PER_CLASS个样本\n",
    "    selected_samples = class_samples[:NUM_PER_CLASS]\n",
    "    filtered_samples.extend(selected_samples)\n",
    "    labels.extend([cls] * len(selected_samples))\n",
    "\n",
    "print(f\"共筛选出 {len(filtered_samples)} 个样本\")\n",
    "\n",
    "# 加载SigLIP模型和图像处理器\n",
    "print(\"加载SigLIP模型...\")\n",
    "model_name = \"google/siglip-base-patch16-224\"\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# 将模型移动到可用设备\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 提取图像特征\n",
    "print(\"提取图像特征...\")\n",
    "embeddings = []\n",
    "\n",
    "# 批量处理图像以提高效率\n",
    "batch_size = 32\n",
    "for i in tqdm(range(0, len(filtered_samples), batch_size)):\n",
    "    batch_samples = filtered_samples[i:i+batch_size]\n",
    "    \n",
    "    # 处理图像\n",
    "    inputs = processor(\n",
    "        images=[sample[\"image\"] for sample in batch_samples],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # 移动到设备并获取特征\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(** inputs)\n",
    "    \n",
    "    # 使用[CLS] token的输出作为图像嵌入\n",
    "    batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    embeddings.extend(batch_embeddings)\n",
    "\n",
    "# 转换为numpy数组\n",
    "embeddings = np.array(embeddings)\n",
    "print(f\"提取的特征形状: {embeddings.shape}\")\n",
    "\n",
    "# 使用UMAP降维到2维（修改UMAP使用方式）\n",
    "print(\"使用UMAP进行降维...\")\n",
    "umap_model = umap.UMAP(n_components=2, random_state=42)  # 这里是修改的关键\n",
    "embeddings_2d = umap_model.fit_transform(embeddings)\n",
    "\n",
    "# 可视化结果\n",
    "print(\"绘制可视化结果...\")\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = sns.scatterplot(\n",
    "    x=embeddings_2d[:, 0], \n",
    "    y=embeddings_2d[:, 1],\n",
    "    hue=labels,\n",
    "    palette=sns.color_palette(\"husl\", len(CLASSES)),\n",
    "    s=50,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "plt.title(\"SigLIP特征的UMAP降维可视化 (Food101的5个类别)\", fontsize=16)\n",
    "plt.xlabel(\"UMAP维度 1\", fontsize=12)\n",
    "plt.ylabel(\"UMAP维度 2\", fontsize=12)\n",
    "plt.legend(title=\"食物类别\", title_fontsize=12, fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# 保存图像\n",
    "plt.savefig(\"food101_siglip_umap.png\", dpi=300, bbox_inches=\"tight\")\n",
    "print(\"可视化结果已保存为 food101_siglip_umap.png\")\n",
    "\n",
    "# 显示图像\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b9d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = [\"SimHei\", \"WenQuanYi Micro Hei\", \"Heiti TC\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# 配置参数\n",
    "CLASSES = [\"pizza\", \"sushi\", \"hamburger\", \"ice_cream\", \"dumplings\"]\n",
    "TRAIN_NUM_PER_CLASS = 1  # 每类使用1张图片训练\n",
    "TEST_NUM_PER_CLASS = 99  # 每类使用99张图片测试\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "LR = 0.001\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LOG_DIR = \"./runs/siglip_linear_probing\"\n",
    "\n",
    "# 创建TensorBoard记录器\n",
    "writer = SummaryWriter(LOG_DIR)\n",
    "\n",
    "# 1. 加载数据集和预处理\n",
    "print(\"加载数据集...\")\n",
    "# 如果之前已下载数据集，可使用load_from_disk加载本地数据\n",
    "try:\n",
    "    dataset = load_dataset(\"food101\", split=\"train\")\n",
    "except:\n",
    "    # 从本地加载（请替换为你的实际路径）\n",
    "    dataset = load_from_disk(\"./food101/train\")\n",
    "\n",
    "# 分割训练集和测试集\n",
    "train_samples = []\n",
    "test_samples = []\n",
    "train_labels = []\n",
    "test_labels = []\n",
    "\n",
    "for cls in CLASSES:\n",
    "    class_idx = dataset.features[\"label\"].str2int(cls)\n",
    "    class_samples = [s for s in dataset if s[\"label\"] == class_idx]\n",
    "    \n",
    "    # 每类第一张作为训练样本，其余作为测试样本\n",
    "    train_samples.append(class_samples[0])\n",
    "    train_labels.append(cls)\n",
    "    test_samples.extend(class_samples[1:TEST_NUM_PER_CLASS+1])\n",
    "    test_labels.extend([cls] * TEST_NUM_PER_CLASS)\n",
    "\n",
    "print(f\"训练集: {len(train_samples)}个样本\")\n",
    "print(f\"测试集: {len(test_samples)}个样本\")\n",
    "\n",
    "# 2. 加载SigLIP模型并提取特征\n",
    "print(\"加载SigLIP模型并提取特征...\")\n",
    "model_name = \"google/siglip-base-patch16-224\"\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# 提取特征的函数\n",
    "def extract_features(samples):\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for sample in tqdm(samples, desc=\"提取特征\"):\n",
    "            inputs = processor(images=sample[\"image\"], return_tensors=\"pt\").to(DEVICE)\n",
    "            outputs = model(**inputs)\n",
    "            # 使用[CLS] token的输出作为特征\n",
    "            embeddings.append(outputs.last_hidden_state[:, 0, :].cpu().numpy()[0])\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# 提取训练集和测试集的特征\n",
    "X_train = extract_features(train_samples)\n",
    "X_test = extract_features(test_samples)\n",
    "\n",
    "# 标签编码\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_labels)\n",
    "y_test = label_encoder.transform(test_labels)\n",
    "\n",
    "# 3. 创建数据集和数据加载器\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = FeatureDataset(X_train, y_train)\n",
    "test_dataset = FeatureDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 4. 定义线性探针模型\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(CLASSES)\n",
    "model_probe = LinearProbe(input_dim, num_classes).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_probe.parameters(), lr=LR)\n",
    "\n",
    "# 5. 训练模型\n",
    "print(\"开始训练线性探针...\")\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model_probe.train()\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_true = []\n",
    "    \n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_probe(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * features.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_true.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # 计算训练集指标\n",
    "    train_loss /= len(train_dataset)\n",
    "    train_acc = accuracy_score(train_true, train_preds)\n",
    "    \n",
    "    # 在测试集上评估\n",
    "    model_probe.eval()\n",
    "    test_loss = 0.0\n",
    "    test_preds = []\n",
    "    test_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            features, labels = features.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model_probe(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item() * features.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_true.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # 计算测试集指标\n",
    "    test_loss /= len(test_dataset)\n",
    "    test_acc = accuracy_score(test_true, test_preds)\n",
    "    \n",
    "    # 记录到TensorBoard\n",
    "    writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Test', test_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/Train', train_acc, epoch)\n",
    "    writer.add_scalar('Accuracy/Test', test_acc, epoch)\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model_probe.state_dict(), \"best_linear_probe.pth\")\n",
    "    \n",
    "    # 打印进度\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "writer.close()\n",
    "print(f\"最佳测试准确率: {best_acc:.4f}\")\n",
    "\n",
    "# 6. 绘制训练曲线\n",
    "# 从TensorBoard日志中读取数据并绘图\n",
    "# 这里简化处理，实际中可以使用tensorboard或读取事件文件\n",
    "print(\"请使用以下命令查看训练曲线:\")\n",
    "print(f\"tensorboard --logdir={LOG_DIR}\")\n",
    "\n",
    "# 绘制简单的训练曲线（需要手动记录数据或从事件文件读取）\n",
    "# 这里仅作示例框架\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 假设我们有记录的loss和acc数据\n",
    "# 实际使用时需要从TensorBoard日志中提取\n",
    "epochs_range = range(1, EPOCHS+1)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, [0]*EPOCHS, label='训练损失')  # 这里替换为实际数据\n",
    "plt.plot(epochs_range, [0]*EPOCHS, label='测试损失')  # 这里替换为实际数据\n",
    "plt.title('损失曲线')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, [0]*EPOCHS, label='训练准确率')  # 这里替换为实际数据\n",
    "plt.plot(epochs_range, [0]*EPOCHS, label='测试准确率')  # 这里替换为实际数据\n",
    "plt.title('准确率曲线')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f069ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 配置API环境变量（确保与提供的密钥和Base URL一致）\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-yxhm7vIgkeffD0FU1bE5F797B654482d94CbB6DbBa556b96\"\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"https://api.ai-gaochao.cn/v1\"\n",
    "\n",
    "# 分类类别（与任务2保持一致）\n",
    "CLASSES = [\"pizza\", \"sushi\", \"hamburger\", \"ice_cream\", \"dumplings\"]\n",
    "TEST_NUM_PER_CLASS = 20  # 每类测试样本数量，控制API消耗\n",
    "\n",
    "# 图像转Base64编码（API要求的图像格式）\n",
    "def image_to_base64(image):\n",
    "    buffered = io.BytesIO()\n",
    "    image.save(buffered, format=\"PNG\")  # 统一保存为PNG，避免编码兼容性问题\n",
    "    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "# 构建提示词（严格约束结构化输出，避免解析失败）\n",
    "def build_prompt():\n",
    "    return f\"\"\"任务：对提供的食物图片进行分类，仅从以下指定类别中选择一个：{', '.join(CLASSES)}。\n",
    "\n",
    "输出要求：\n",
    "1. 仅返回JSON格式结果，不添加任何额外文字、解释或注释；\n",
    "2. JSON必须只包含一个键\"category\"，其值为上述类别中的一个（严格匹配，不可自定义）；\n",
    "3. 若无法明确判断，需选择最可能的类别。\n",
    "\n",
    "示例输出（仅参考格式）：\n",
    "{{\"category\": \"pizza\"}}\n",
    "\"\"\"\n",
    "\n",
    "# 异步调用API（单个图像分类的基础单元）\n",
    "async def classify_image(session, image, prompt):\n",
    "    base64_image = image_to_base64(image)\n",
    "    \n",
    "    # 构造API请求体（适配Gemini通过OpenAI兼容接口的调用格式）\n",
    "    payload = {\n",
    "        \"model\": \"gemini-2.5-flash\",  # 模型名称需与API支持的一致\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},  # 文本指令\n",
    "                    {\n",
    "                        \"type\": \"image_url\",  # 图像数据\n",
    "                        \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"}\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.0,  # 降低随机性，确保分类结果稳定\n",
    "        \"max_tokens\": 50      # 限制输出长度，避免冗余内容导致解析失败\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        async with session.post(\n",
    "            url=f\"{os.environ['OPENAI_BASE_URL']}/chat/completions\",\n",
    "            headers={\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Authorization\": f\"Bearer {os.environ['OPENAI_API_KEY']}\"\n",
    "            },\n",
    "            json=payload,\n",
    "            timeout=25  # 超时时间，避免长期阻塞\n",
    "        ) as response:\n",
    "            if response.status != 200:\n",
    "                # 打印API错误详情，便于排查问题（如密钥无效、限流等）\n",
    "                error_msg = await response.text()\n",
    "                print(f\"API调用失败：状态码{response.status}，详情：{error_msg}\")\n",
    "                return None\n",
    "            \n",
    "            # 解析API响应（严格按JSON格式提取结果）\n",
    "            data = await response.json()\n",
    "            content = data['choices'][0]['message']['content'].strip()  # 去除首尾空白字符\n",
    "            result = json.loads(content)  # 解析JSON\n",
    "            return result['category'] if 'category' in result else None\n",
    "    \n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"解析模型输出失败：模型返回非JSON内容 -> {content}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"分类过程出错：{str(e)}\")\n",
    "        return None\n",
    "\n",
    "# 异步批量处理（控制并发数，避免API限流）\n",
    "async def batch_classify(images, labels, prompt, batch_size=5):\n",
    "    results = []\n",
    "    # 创建异步会话（复用连接，提升效率）\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # 按批次处理，每批并发调用API\n",
    "        for i in tqdm(range(0, len(images), batch_size), desc=\"分类进度\"):\n",
    "            batch_images = images[i:i+batch_size]\n",
    "            batch_labels = labels[i:i+batch_size]\n",
    "            \n",
    "            # 创建当前批次的所有异步任务\n",
    "            tasks = [classify_image(session, img, prompt) for img in batch_images]\n",
    "            # 等待当前批次所有任务完成（控制并发量）\n",
    "            batch_results = await asyncio.gather(*tasks)\n",
    "            \n",
    "            # 记录每一个样本的预测结果和真实标签\n",
    "            for pred, true in zip(batch_results, batch_labels):\n",
    "                results.append({\n",
    "                    \"predicted\": pred,\n",
    "                    \"true\": true\n",
    "                })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 主异步函数（数据准备 + 分类 + 结果计算）\n",
    "async def main():\n",
    "    # 1. 加载数据集（优先Hub，失败则本地加载）\n",
    "    print(\"加载Food101数据集...\")\n",
    "    try:\n",
    "        dataset = load_dataset(\"food101\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        print(f\"从Hugging Face Hub加载失败，尝试本地加载：{str(e)}\")\n",
    "        from datasets import load_from_disk\n",
    "        # 注意：需将路径替换为你本地Food101数据集的实际解压路径\n",
    "        dataset = load_from_disk(\"./food101/train\")#没下...网不行,下不来\n",
    "    \n",
    "    # 2. 准备测试集\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    for cls in CLASSES:\n",
    "        class_idx = dataset.features[\"label\"].str2int(cls)  # 类别名转索引\n",
    "        class_samples = [s for s in dataset if s[\"label\"] == class_idx]  # 筛选该类样本\n",
    "        \n",
    "        # 确保样本数量足够，避免索引越界\n",
    "        if len(class_samples) < 10 + TEST_NUM_PER_CLASS:\n",
    "            print(f\"警告：{cls}类样本不足，仅能提供{len(class_samples)-10}个测试样本（需{TEST_NUM_PER_CLASS}个）\")\n",
    "            selected_samples = class_samples[10:]  # 取剩余所有样本\n",
    "        else:\n",
    "            selected_samples = class_samples[10:10+TEST_NUM_PER_CLASS]  # 取第11-30张\n",
    "        \n",
    "        # 收集图像和标签\n",
    "        test_images.extend([s[\"image\"] for s in selected_samples])\n",
    "        test_labels.extend([cls] * len(selected_samples))\n",
    "    \n",
    "    # 检查测试集是否为空\n",
    "    if len(test_images) == 0:\n",
    "        print(\"错误：未收集到任何测试样本，请检查数据集路径或类别筛选逻辑\")\n",
    "        return\n",
    "    print(f\"测试集准备完成：共{len(test_images)}个样本（{len(CLASSES)}类，每类约{TEST_NUM_PER_CLASS}个）\")\n",
    "    \n",
    "    # 3. 构建提示词\n",
    "    prompt = build_prompt()\n",
    "    \n",
    "    # 4. 执行异步批量分类\n",
    "    print(\"开始调用Gemini 2.5 Flash API进行分类...\")\n",
    "    results = await batch_classify(test_images, test_labels, prompt, batch_size=5)\n",
    "    \n",
    "    # 5. 计算准确率（过滤预测失败的样本）\n",
    "    # 筛选有效结果（排除预测为None的样本）\n",
    "    valid_results = [r for r in results if r[\"predicted\"] is not None]\n",
    "    if len(valid_results) == 0:\n",
    "        print(\"错误：所有样本分类失败，无法计算准确率（检查API密钥、Base URL或网络）\")\n",
    "        return\n",
    "    \n",
    "    # 提取真实标签和预测标签\n",
    "    y_true = [r[\"true\"] for r in valid_results]\n",
    "    y_pred = [r[\"predicted\"] for r in valid_results]\n",
    "    accuracy = accuracy_score(y_true, y_pred)  # 计算准确率\n",
    "    \n",
    "    # 6. 输出结果\n",
    "    print(f\"\\n=== 分类结果汇总 ===\")\n",
    "    print(f\"总测试样本数：{len(results)}\")\n",
    "    print(f\"有效分类样本数：{len(valid_results)}\")\n",
    "    print(f\"最终分类准确率：{accuracy:.4f}（正确{sum([p==t for p,t in zip(y_pred, y_true)])}个 / 有效{len(valid_results)}个）\")\n",
    "    \n",
    "    # 7. 保存结果到JSON文件（便于后续分析）\n",
    "    with open(\"gemini_classification_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\n分类详情已保存到：gemini_classification_results.json\")\n",
    "    \n",
    "    # 8. 与任务2（SigLIP+线性探针）对比分析\n",
    "    print(f\"\\n=== 与任务2（SigLIP+线性探针）准确率对比 ===\")\n",
    "    print(\"1. 通常情况下，Gemini 2.5 Flash准确率更高：\")\n",
    "    print(\"   - 原因1：Gemini具备视觉-语言联合理解能力，能处理模糊/复杂背景（如披萨与汉堡的芝士、面包区分）；\")\n",
    "    print(\"   - 原因2：Gemini内置食物类别知识，无需额外训练线性层，减少小样本过拟合；\")\n",
    "    print(\"   - 原因3：抗干扰能力更强（如冰淇淋融化、饺子摆盘变化对分类影响更小）。\")\n",
    "    print(\"2. Gemini的劣势：\")\n",
    "    print(\"   - 依赖API调用，存在延迟（单样本约0.5-2秒）和成本（按token/图像计费）；\")\n",
    "    print(\"   - 无法本地化部署，大规模数据分类（如万级样本）成本过高；\")\n",
    "    print(\"   - 受API限流影响，并发量不能无限制提升。\")\n",
    "\n",
    "# -------------------------- 关键修复：启动异步函数 --------------------------\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e12501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
